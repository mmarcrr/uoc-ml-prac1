---
title: "Predicción de los splice junctions"
author: "Marc Rodriguez"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document:
    toc: yes
  html_document:
    toc: yes
params:
    file_in: "splice.txt"
    kvalues: !r c(1, 5, 11, 21, 51, 71)

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r libraries, include=FALSE}
# Load packages
library(knitr)
library(stringr)
library(class)
library(gmodels)
library(kableExtra)
library(ROCR)
library(ggseqlogo)

```

El algoritmo kNN utiliza el método de clasificación de vecinos más cercanos. Las fortalezas y debilidades de este algoritmo son:

| **Fortalezas**    | **Debilidades**  | 
| ------------------------ |:------------------------------------------------------- |
|* Simple y eficaz  |* No produce un modelo, que limita la capacidad de encontrar conocimientos novedosos en relaciones entre características
|* No hace suposiciones sobre la distribución de datos subyacente | * Fase de clasificación lenta
|* Fase de entrenamiento ésrápida | * Requiere una gran cantidad de memoria
| | * Características nominales y datos faltantes requieren procesamiento adicional

El algoritmo kNN comienza con un conjunto de datos de entrenamiento compuesto por ejemplos que son clasificados en varias categorías, etiquetadas por una variable nominal. Supongamos que nosotros tener un conjunto de datos de prueba que contenga ejemplos sin etiquetar que, de lo contrario, tengan el mismo características como los datos de entrenamiento. Para cada registro en el conjunto de datos de prueba, kNN identifica k registros en los datos de entrenamiento que son los "más cercanos" en similitud, donde k es un número entero especificado de antemano. A la instancia de prueba sin etiqueta se le asigna la clase de la mayoría de los k vecinos mas cercanos.

Apesar de ser un algorimos simple, es capaz de abordar tareas extremadamente complejas, como identificar masas cancerosas. 


##Predicción de los splice junctions
Los splice junctions son puntos en una secuencia de ADN en los que se elimina el ADN “superfluo” durante el proceso de síntesis de proteínas en organismos superiores. El problema que se plantea en este conjunto de datos es reconocer, dada una secuencia de ADN, los límites entre los exones (las partes de la secuencia de ADN retenidas después del splicing) y los intrones (las partes de la secuencia de ADN que se cortan). Este problema consta de dos subtareas: reconocer los límites exón/intrón (denominados sitios EI) y reconocer los límites intrón/exón (sitios IE). En la comunidad biológica, las fronteras de la IE se denominan acceptors, mientras que las fronteras de la EI se denominan donors. Todos los ejemplos fueron tomados de Genbank 64.1. Las categorías “EI” e “IE” incluyen “genes con splicing” de los primates en Genbank 64.1. Los ejemplos de no splicing fueron tomados de secuencias que se sabe que no incluyen un sitio de splicing. Los datos estan disponibles en la PEC en el fichero splice.txt. El archivo contiene 3190 filas que corresponden a las distintas secuencias, y 3 columnas separadas por coma. La primera columna correspondiente a la clase de la secuencia (EI, IE o N), la segunda columna con el nombre identificador de la secuencia y la tercera columna  con la secuencia propiamente. Tratandose de secuencias de ADN, apareceran los nucleótidos identificados de
manera estandar con las letras A, G, T y C. Además, aparecen otros caracteres entre los caracteres estándar,
D, N, S y R, que indican ambigüedad según la siguiente tabla:
|**caracter** | **significado** |
|---------|---------------|
|D | A o G o T |
|N | A o G o C o T |
|S | C o G |
|R | A o G |
La manera elegida para representar los datos es un paso crucial en los algoritmos. En el caso que nos ocupa, análisis basados en secuencias, se usará la codificación one-hot. La codificación one-hot representa cada nucleótido por un vector de 8 componentes, con 7 de ellas a 0 y una a 1. Pongamos por ejemplo, el nucleótido A se representa por (1,0,0,0,0,0,0,0), el nucleótido G por (0,1,0,0,0,0,0,0), el T por (0,0,1,0,0,0,0,0) y, finalmente, la C por (0,0,0,1,0,0,0,0) y los caracteres de ambiguedad los representaremos, la D por (0,0,0,0,1,0,0,0), la N por (0,0,0,0,0,1,0,0), la S por (0,0,0,0,0,0,1,0) y la R por (0,0,0,0,0,0,0,1).

## función en R one-hot encoding

función en R que implementa la codificación “one-hot” (one-hot encoding) de las secuencias, Recive como paramentro la sequencia y el espacio de posibles caracteres
```{r one-hote}
space<-c("A","G","T","C","D","N","S","R")
encode.onehot<-function(sequence,space){
  seq.splited<-unlist(strsplit(str_trim(sequence),""))
  sapply(seq.splited,function(code){match(space,code,nomatch=0)}, simplify = TRUE)
}
```
Ejemplo de apliacacion monstrando el resultado como vector 

```{r one-hote-example}
nuc_seq="CCAGCTGCATCACAGGAGGCCAGCGAGCAGGTCTGTTCCAAGGGCCTTCGAGCCAGTCTG"
c(t(encode.onehot(nuc_seq,space)))
```
##Desarrollo de un script en R que implementa un clasificador knn. 
###(A1) Leer los datos del fichero splice.txt 
Leemos el fichero `r params$file_in` que es un parametro del documento 
```{r read}
data_in <- read.csv(file.path(params$file_in), header=FALSE)
head(data_in)
```
###(A2)breve descripción de los datos
```{r}
str(data_in)
summary(data_in)
```
```{r}

mysplice<-data.frame(data_in[1],data_in[2],t(apply(data_in[3],1,encode.onehot,space)))
colnames(mysplice)[1] <- "class"
colnames(mysplice)[2] <- "seq_name"
mysplice$V1<-factor(mysplice$V1, levels = c("EI","IE","N"))

```


```{r}

EIN.data <- subset(mysplice,mysplice$class %in% c("N","EI"))
IEN.data <- subset(mysplice,mysplice$class %in% c("N","IE"))
EIN.onehot <-subset(EIN.data,select=-c(1,2))
IEN.onehot <-subset(IEN.data,select=-c(1,2))
```
```{r}
################## data spliting
EIN.train<-sample(1:nrow(EIN.onehot),round(2*nrow(EIN.onehot)/3,0))
IEN.train<-sample(1:nrow(EIN.onehot),round(2*nrow(IEN.onehot)/3,0))

```


```{r}
# create training and test data
EIN.training<-EIN.onehot[EIN.train,]
EIN.test<-EIN.onehot[-EIN.train,]
IEN.training<-IEN.onehot[IEN.train,]
IEN.test<-IEN.onehot[-IEN.train,]

# create labels for training and test data
EIN.class_training<-EIN.data[EIN.train,1]
EIN.class_test<-EIN.data[-EIN.train,1]
# create labels for training and test data
IEN.class_training<-IEN.data[IEN.train,1]
IEN.class_test<-IEN.data[-IEN.train,1]

```
```{r}

set.seed(123) #fijar la semilla para el inicio del clasificador
EIN.test_pred <- knn(train =EIN.training, test = EIN.test, cl = EIN.class_training, k=20)
IEN.test_pred <- knn(train =IEN.training, test = IEN.test, cl = IEN.class_training, k=20)

```


```{r}
# load the "gmodels" library
# Create the cross tabulation of predicted vs. actual
############ evaluating model performance
CrossTable(x = EIN.class_test, y = EIN.test_pred , prop.chisq=FALSE)
```
```{r}
# load the "gmodels" library
# Create the cross tabulation of predicted vs. actual
############ evaluating model performance
CrossTable(x = IEN.class_test, y = IEN.test_pred , prop.chisq=FALSE)
```
```{r}
load("splice_oh.Rdata")
splice_subset<-subset(output,select=-c(1,2))
```
```{r}

testknn <-function(data.training,data.test,data.class_training,data.class_test,ks){
  
  resum <- data.frame(ks, FN=NA, FP=NA, mal_clas=NA)
  j <- 0
  for (i in ks){
    j <- j +1
    class_test_pred <-knn(train =data.training, test = data.test, cl = data.class_training, k=i)
    conf.mat <- CrossTable(x = data.class_test, y = class_test_pred, prop.chisq=FALSE)
    resum[j,2:4] <- c(conf.mat$t[2,1], conf.mat$t[1,2], ((conf.mat$t[1,2]+conf.mat$t[2,1])/sum(conf.mat$t))*100)
  }
  resum
}
```


```{r}
require(knitr, quietly = TRUE)
EIN.resum<-testknn(EIN.training,EIN.test,EIN.class_training,EIN.class_test,params$kvalues)
kable(EIN.resum, col.names=c("valor k", "# falsos negativos","# falsos positivos", "% mal clasificados"),
align= c("l","c","c","c"), caption= paste("Algoritmo kNN: ",params$file_in ,sep=""))
```
```{r}
require(knitr, quietly = TRUE)
IEN.resum<-testknn(IEN.training,IEN.test,IEN.class_training,IEN.class_test,params$kvalues)
kable(IEN.resum, col.names=c("valor k", "# falsos negativos","# falsos positivos", "% mal clasificados"),align= c("l","c","c","c"), caption= paste("Algoritmo kNN: ",params$file_in ,sep=""))

```
```{r}
# try several different values of k
)

myROC <-function(data.training,data.test,data.class_training,data.class_test,ks){
  
  par(mfrow=c(3,2))
  for (i in ks){
    test_pred <-knn(train = data.training, test = data.test, cl = data.class_training, k=i,prob = TRUE)
    prob <- attr(test_pred, "prob")
    prob1 <- ifelse(test_pred == "N" , prob, 1-prob)
    res <- auc(data.class_test,prob1)

  pred_knn <- ROCR::prediction(prob1, data.class_test)
  pred_knn <- performance(pred_knn, "tpr", "fpr")
  plot(pred_knn, avg= "threshold", colorize=T, lwd=3, main=paste("ROC curve, k: ", i, ", auc=", round(res,4)))
  }
}

```
```{r}
ks <- c(1, 5, 11, 21, 51, 71)
myROC(EIN.training,EIN.test,EIN.class_training,EIN.class_test,ks)

```
```{r}
myROC(IEN.training,IEN.test,IEN.class_training,IEN.class_test,ks)

```
```{r}

ggseqlogo(str_trim(subset(data_in, V1 == "IE")$V3))

```
```{r}
ggseqlogo(str_trim(subset(data_in, V1 == "EI")$V3))
```


```{r}
ggseqlogo(str_trim(subset(data_in, V1 == "N")$V3))
```

