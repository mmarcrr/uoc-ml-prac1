---
title: "Predicción de los splice junctions"
author: "Marc Rodriguez"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    toc: yes
    toc_depth: 2
    toc_float: yes
  pdf_document:
    toc: yes
    keep_tex: yes
    toc_depth: 2
params:
    file_in: "splice.txt"
    kvalues: !r c(1, 5, 11, 21, 51, 71)
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r libraries, include=FALSE}
# Load packages
library(knitr)
library(stringr)
library(class)
library(gmodels)
library(kableExtra)
library(ROCR)
library(pROC)

library(ggseqlogo)
require(knitr, quietly = TRUE)

```

El algoritmo kNN utiliza el método de clasificación de vecinos más cercanos. Las fortalezas y debilidades de este algoritmo son:

| **Fortalezas**    | **Debilidades**  | 
| ------------------------ |:------------------------------------------------------- |
|* Simple y eficaz  |* No produce un modelo, que limita la capacidad de encontrar conocimientos novedosos en relaciones entre características
|* No hace suposiciones sobre la distribución de datos subyacente | * Fase de clasificación lenta
|* Fase de entrenamiento ésrápida | * Requiere una gran cantidad de memoria
| | * Características nominales y datos faltantes requieren procesamiento adicional

El algoritmo kNN comienza con un conjunto de datos de entrenamiento compuesto por ejemplos que son clasificados en varias categorías, etiquetadas por una variable nominal. Supongamos que nosotros tener un conjunto de datos de prueba que contenga ejemplos sin etiquetar que, de lo contrario, tengan el mismo características como los datos de entrenamiento. Para cada registro en el conjunto de datos de prueba, kNN identifica k registros en los datos de entrenamiento que son los "más cercanos" en similitud, donde k es un número entero especificado de antemano. A la instancia de prueba sin etiqueta se le asigna la clase de la mayoría de los k vecinos mas cercanos.

Apesar de ser un algorimos simple, es capaz de abordar tareas extremadamente complejas, como identificar masas cancerosas. 


## Predicción de los splice junctions
Los splice junctions son puntos en una secuencia de ADN en los que se elimina el ADN “superfluo” durante el proceso de síntesis de proteínas en organismos superiores. El problema que se plantea en este conjunto de datos es reconocer, dada una secuencia de ADN, los límites entre los exones (las partes de la secuencia de ADN retenidas después del splicing) y los intrones (las partes de la secuencia de ADN que se cortan). Este problema consta de dos subtareas: reconocer los límites exón/intrón (denominados sitios EI) y reconocer los límites intrón/exón (sitios IE). En la comunidad biológica, las fronteras de la IE se denominan acceptors, mientras que las fronteras de la EI se denominan donors.

Todos los ejemplos fueron tomados de Genbank 64.1. Las categorías “EI” e “IE” incluyen “genes con splicing” de los primates en Genbank 64.1. Los ejemplos de no splicing fueron tomados de secuencias que se sabe que no incluyen un sitio de splicing. Los datos estan disponibles en la PEC en el fichero splice.txt. El archivo contiene 3190 filas que corresponden a las distintas secuencias, y 3 columnas separadas por coma. La primera columna correspondiente a la clase de la secuencia (EI, IE o N), la segunda columna con el nombre identificador de la secuencia y la tercera columna  con la secuencia propiamente. Tratandose de secuencias de ADN, apareceran los nucleótidos identificados de
manera estandar con las letras A, G, T y C. Además, aparecen otros caracteres entre los caracteres estándar,
D, N, S y R, que indican ambigüedad según la siguiente tabla:

| **caracter** | **significado** |
| ------------ |:--------------- |
| D | A o G o T |
| N | A o G o C o T |
| S | C o G |
| R | A o G |

La manera elegida para representar los datos es un paso crucial en los algoritmos. En el caso que nos ocupa, análisis basados en secuencias, se usará la codificación one-hot. La codificación one-hot representa cada nucleótido por un vector de 8 componentes, con 7 de ellas a 0 y una a 1. Pongamos por ejemplo, el nucleótido A se representa por (1,0,0,0,0,0,0,0), el nucleótido G por (0,1,0,0,0,0,0,0), el T por (0,0,1,0,0,0,0,0) y, finalmente, la C por (0,0,0,1,0,0,0,0) y los caracteres de ambiguedad los representaremos, la D por (0,0,0,0,1,0,0,0), la N por (0,0,0,0,0,1,0,0), la S por (0,0,0,0,0,0,1,0) y la R por (0,0,0,0,0,0,0,1).


## función en R one-hot encoding

función en R que implementa la codificación “one-hot” (one-hot encoding) de las secuencias, Recive como paramentro la sequencia y el espacio de posibles caracteres
```{r one-hote}
space<-c("A","G","T","C","D","N","S","R")
encode.onehot<-function(sequence,space){
  seq.splited<-unlist(strsplit(str_trim(sequence),""))
  sapply(seq.splited,function(code){match(space,code,nomatch=0)}, simplify = TRUE)
}
```

Ejemplo de apliacacion monstrando el resultado como vector 

```{r one-hote-example}
nuc_seq="CCAGCTGCATCACAGGAGGCCAGCGAGCAGGTCTGTTCCAAGGGCCTTCGAGCCAGTCTG"
c(t(encode.onehot(nuc_seq,space)))
```

## Desarrollo de un script en R que implementa un clasificador knn. 
### (A1) Leer los datos del fichero splice.txt 

Leemos el fichero `r params$file_in` que es un parametro del documento

```{r read}
data_in <- read.csv(file.path(params$file_in), header=FALSE)

```

### (A2)breve descripción de los datos
Observamos algunos valores del fichero
```{r}
head(data_in)
```
vemos una descripcion de los datos
```{r}
str(data_in)
```
Los datos leidos del fichero `r params$file_in` contiene 3190 filas que corresponden a las distintas secuencias, y 3 columnas separadas por coma. La primera columna correspondiente a la clase de la secuencia (EI, IE o N), la segunda columna con el nombre identificador de la secuencia y la tercera columna  con la secuencia propiamente.

Analizamos la cantidad de secuencias de cada clase, nos dara una idea del balanceo de los datos,
```{r}
knitr::kable(table(data_in[,1]), "pipe",caption="Tabla del número de sequencia de nucleotidos según la clase")
```
### (B) Transformar las secuencias de nucleótidos en vectores numéricos usando la función de transformación desarrollada anteriormente. 

Generamos el dataframe los mas parecido al fichor splice_oh.RData aplicando la funcion onehot creada anteriormente.
Tambien se renombral las columans v1 por class y v2 por seq_name
```{r}
mysplice<-data.frame(data_in[1],data_in[2],t(apply(data_in[3],1,encode.onehot,space)))
colnames(mysplice)[1] <- "class"
colnames(mysplice)[2] <- "seq_name"
```

### (C)Para el subset formado por las secuencias de las clases “EI” y “N”, y para el subset formado por las secuencias de las clases “IE” y “N”, realizar la implementación del algoritmo knn

Creamos los subset de "EI"-"N" y "IE"-"N" 
```{r}
EIN.data <- subset(mysplice,mysplice$class %in% c("N","EI"))
IEN.data <- subset(mysplice,mysplice$class %in% c("N","IE"))
```
Aplicamos la funcion onehot a la columna 3 que es la que contiene las sequencias.  

```{r}
EIN.onehot <-subset(EIN.data,select=-c(1,2))
IEN.onehot <-subset(IEN.data,select=-c(1,2))
```
Utilizando la semilla aleatoria 123 para separar los datos en dos partes, una parte para training
(67%) y una parte para test (33%).
```{r}
set.seed(123)

#generamos un sample de  2/3
EIN.train<-sample(1:nrow(EIN.onehot),round(nrow(EIN.onehot)*2/3,0))
IEN.train<-sample(1:nrow(EIN.onehot),round(nrow(IEN.onehot)*2/3,0))
EIN.train
```

Usamos la lista quenerada con la funcion sample de 2/3 de las posiciones para crear los subsets de entranamiento de EI-N y IE-N
```{r}
EIN.training<-EIN.onehot[EIN.train,]
IEN.training<-IEN.onehot[IEN.train,]

```
El resto de valores los usaremos para el subset de test

```{r}
EIN.test<-EIN.onehot[-EIN.train,]
IEN.test<-IEN.onehot[-IEN.train,]
```


Creamos las etiquetas de entrenamiento y validacion de las clases
```{r}
EIN.class_training<-EIN.data[EIN.train,1]
EIN.class_test<-EIN.data[-EIN.train,1]
IEN.class_training<-IEN.data[IEN.train,1]
IEN.class_test<-IEN.data[-IEN.train,1]

```

Aplicamos la funcion knn sobre los datos dividos para ver el funcionamento
```{r}
EIN.test_pred <- knn(train =EIN.training, test = EIN.test, cl = EIN.class_training, k=20)
IEN.test_pred <- knn(train =IEN.training, test = IEN.test, cl = IEN.class_training, k=20)

```
Generamos la tabla para ver el rendimiento  del algoritmo

primero sobre las clases EI N
```{r}
CrossTable(x = EIN.class_test, y = EIN.test_pred , prop.chisq=FALSE)
```
Y Sobre las clases IE N
```{r}
CrossTable(x = IEN.class_test, y = IEN.test_pred , prop.chisq=FALSE)
```
### (C)Aplicar el knn (k = `r params$ksvalues`) basado en el training para predecir que secuencias del test son secuencias con puntos de splicing (splice junctions) o no. Además, realizar una curva ROC para cada k y mostrar el valor de AUC.

Creamos una funcion para representar las curpas ROC y mostrar el valor de AUC para los k valores `r params$kvalues` 
```{r}
myROC <-function(data.training,data.test,data.class_training,data.class_test,ks){
  par(mfrow=c(3,2))
  for (i in ks){
    test_pred <-knn(train = data.training, test = data.test, cl = data.class_training, k=i,prob = TRUE)
    prob <- attr(test_pred, "prob")
    prob1 <- ifelse(test_pred == "N" , prob, 1-prob)
    res <- auc(data.class_test,prob1)
    pred_knn <- ROCR::prediction(prob1, data.class_test)
    pred_knn <- performance(pred_knn, "tpr", "fpr")
    plot(pred_knn, avg= "threshold", colorize=T, lwd=3, main=paste("ROC curve, k: ", i, ", auc=", round(res,5)))
  }
}

```
Hemos parametrizado las diferentes ks proporcionadas por el enunciado para que sea aplicable el mismo informe a diferentes configuraciones
mostramos en los graficos el valor de "auc" que es el area por debajo de la curva ROC. Que nos da una idea de como de buena es la clasificacion de los valores. 

Primero mostramos las curvas de EI-N
```{r}
myROC(EIN.training,EIN.test,EIN.class_training,EIN.class_test,params$kvalues)
```
A continuzcoin mostramos las curvas IE-N
```{r}
myROC(IEN.training,IEN.test,IEN.class_training,IEN.class_test,params$kvalues)

```
como se intuye en los graficos vemos que el valor de auc más cercano a 1 se encuentra con k=21

## Representar la sequencia logo de las tres clases de secuencias.
```{r}

ggseqlogo(str_trim(subset(data_in, V1 == "IE")$V3))

```
```{r}
ggseqlogo(str_trim(subset(data_in, V1 == "EI")$V3))
```


```{r}
ggseqlogo(str_trim(subset(data_in, V1 == "N")$V3))
```

