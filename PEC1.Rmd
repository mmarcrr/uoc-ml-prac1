---
title: "Algoritmo k-NN"
author: "Marc Rodriguez"
date: "10/24/2021"
output: html_document
params:
    file_in: "splice.txt"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## implementacion codificación “one-hot”

```{r one-hote}
library(stringr)

encode.onehot<-function(sequence){
codes<-c("A","G","T","C","D","N","S","R")
seq.splited<-unlist(strsplit(str_trim(sequence),""))
c(t(sapply(seq.splited,function(x){match(codes,x,nomatch=0)})))
}

nuc_seq="CCAGCTGCATCACAGGAGGCCAGCGAGCAGGTCTGTTCCAAGGGCCTTCGAGCCAGTCTG"
example<-encode.onehot(nuc_seq)
typeof(example)


```


```{r one-hote2}
library(stringr)
#seq.splited<-unlist(strsplit(nuc_seq,""))
#output<-sapply(seq.splited,function(seq){match(codes,seq,nomatch=0)})
#c(t(output))
(nuc_seq)
```


```{r read}
file1="splice.txt"
data_in <- read.csv(file.path(params$file_in), header=FALSE)
data_in
```
```{r}
summary(data_in)
```
```{r}
encode.onehot<-function(sequence){
codes<-c("A","G","T","C","D","N","S","R")
seq.splited<-unlist(strsplit(str_trim(sequence),""))
sapply(seq.splited,function(x){match(codes,x,nomatch=0)}, simplify = TRUE)
}

mysplice<-data.frame(data_in[1],data_in[2],t(apply(data_in[3],1,encode.onehot)))
colnames(mysplice)[1] <- "class"
colnames(mysplice)[2] <- "seq_name"


```


```{r}
encode.source <- function(data){
  data.frame(data[1],data[2],t(apply(data[3],1,encode.onehot)))
}

output<-encode.source(data_in)
out_subset.EIN <- subset(output,output$class %in% c("N","EI"))
out_subset.IEN <- subset(output,output$class %in% c("N","II"))
out_subset.EIN.onehot <-subset(out_subset.EIN,select=-c(1,2))
out_subset.IEN.onehot <-subset(out_subset.IEN,select=-c(1,2))
```
```{r}
################## data spliting
out_subset.EIN.train<-sample(1:nrow(out_subset.EIN),round(2*nrow(out_subset.EIN)/3,0))
out_subset.IEN.train<-sample(1:nrow(out_subset.IEN),round(2*nrow(out_subset.IEN)/3,0))
```


```{r}
# create training and test data
out_subset.EIN.training<-out_subset.EIN[out_subset.EIN.train,]
out_subset.EIN.test<-out_subset[-out_subset.EIN.train,]
out_subset.IEN.training<-out_subset.IEN[out_subset.IEN.train,]
out_subset.IEN.test<-out_subset[-out_subset.IEN.train,]

# create labels for training and test data
out_subset.EIN.class_training<-out_subset.EIN[out_subset.EIN.train,1]
out_subset.EIN.class_test<-out_subset.EIN[-out_subset.EIN.train,1]
# create labels for training and test data
out_subset.IEN.class_training<-out_subset.IEN[out_subset.IEN.train,1]
out_subset.IEN.class_test<-out_subset.IEN[-out_subset.IEN.train,1]

```
```{r}
############### libraries loading
library(class) # knn
############## data prediction

set.seed(123) #fijar la semilla para el inicio del clasificador
test_pred <- knn(train =out_subset.EIN.training, test = out_subset.EIN.test, cl = out_subset.EIN.class_training, k=20)
```
```{r}
load("splice_oh.Rdata")
splice_subset<-subset(output,select=-c(1,2))
```


```{r}
################## data spliting
set.seed(123) #fijar la semilla para el generador pseudoaleatorio
train<-sample(1:nrow(splice_subset),round(2*nrow(splice_subset)/3,0))
# create training and test data
out_training<-splice_subset[train,]
out_test<-splice_subset[-train,]
# create labels for training and test data
class_training<-output[train,1]
class_test<-output[-train,1]

```

```{r}
############### libraries loading
library(class) # knn
############## data prediction

set.seed(123) #fijar la semilla para el inicio del clasificador
test_pred <- knn(train =out_training, test = out_test, cl = class_training, k=3)
```

```{r}
# load the "gmodels" library
library(gmodels)
# Create the cross tabulation of predicted vs. actual
############ evaluating model performance
CrossTable(x = class_test, y = test_pred , prop.chisq=FALSE)
```
```{r}

ks <- c(1, 5, 11, 21, 51, 71)
resum <- data.frame(ks, FN=NA, FP=NA, mal_clas=NA)

j <- 0
for (i in ks){
  j <- j +1
  class_test_pred <-knn(train =out_training, test = out_test, cl = class_training, k=i)
  conf.mat <- CrossTable(x = class_test, y = class_test_pred, prop.chisq=FALSE)
  
  resum[j,2:4] <- c(conf.mat$t[2,1], conf.mat$t[1,2], ((conf.mat$t[1,2]+conf.mat$t[2,1])/sum(conf.mat$t))*100)
}
```


```{r}
require(knitr, quietly = TRUE)
kable(resum, col.names=c("valor k", "# falsos negativos","# falsos positivos", "% mal clasificados"),
align= c("l","c","c","c"), caption= paste("Algoritmo kNN: ",params$file_in ,sep=""))
```
```{r}
# try several different values of k
library(pROC)
ks <- c(1, 5, 11, 21, 51, 71)
par(mfrow=c(2,2))
for (i in ks){
  test_pred <- knn(train =out_training, test = out_test, cl = class_training, k=i, prob=TRUE)
  prob <- attr(test_pred, "prob")
  # convert the proportion of the votes for the winning class to p(test_pred == "SB")
  prob1 <- ifelse(test_pred == "EI", prob, 1-prob)
  
  res <- auc(class_test,prob1)
}
pred_knn <- ROCR::prediction(prob1, class_test)
pred_knn <- performance(pred_knn, "tpr", "fpr")
plot(pred_knn, avg= "threshold", colorize=T, lwd=3, main=paste("ROC curve, k: ", i, ", auc=", round(res,4)))
```

